# Master's Course Project and Thesis Portfolio  
This repository contains selected project reports and a master's thesis overview from my master's studies in Language Technology at Uppsala University. These works reflect my academic background and research interests in natural language processing (NLP), machine learning, large language model efficiency, and human-machine comparison and collaboration.

Each document addresses a unique challenge in the field, blending theoretical insight with hands-on experimentation using modern tools and models.

---

## ğŸ“„ Included Projects

### 1. **Survey on Generating Sequences with Recurrent Neural Networks**
An in-depth literature review covering foundational and advanced techniques in sequence generation using RNNs, with a focus on handwriting synthesis.

- **Focus**: RNN architectures, sequence generation, VAEs
- **Type**: Survey report  
- **Tools**: N/A (theoretical)
- **Path**: `./seq_generation`

---

### 2. **Cross-Lingual Dependency Parsing for Faroese with UUParser**
A practical project exploring the performance of a cross-lingual dependency parser (UUParser) applied to a low-resource language (Faroese). UUParserâ€™s effectiveness with transfer learning techniques for Faroese on various training datasets is evaluated.

- **Focus**: Cross-lingual parsing, low-resource language NLP
- **Type**: Experimental study
- **Tools**: UUParser, Universal Dependencies
- **Path**: `./parsing`

---

### 3. **Syntactic Generalization Under Constraints: Compact Transformer Models and Limited Linguistic Input**
A research project inspired by the "poverty of the stimulus" in human language learning. Compact transformer models trained on limited input data are evaluated to demonstrate their ability to generalize syntactic patterns.

- **Focus**: Language model evaluation, syntactic competence, generalization
- **Type**: Experimental research
- **Tools**: LTGBERT, BLiMP, PyTorch, Hugging Face, Transformers
- **Path**: `./babylm`

---

### 4. **Master's Thesis Overview (Multilingual NLP Efficiency through Model Compression and Adaptive Inference: Knowledge Distillation and Early Exit on XLM-R)**
This master's thesis investigates methods to improve the efficiency of a large multilingual transformer model (XLM-R) without sacrificing cross-lingual generalization performance much. Specifically, the project focuses on compressing XLM-R Large into XLM-R Base using knowledge distillation, and applying early exit mechanism as an adaptive inference technique to further reduce inference cost.

- **Focus**: Multilingual NLP, knowledge distillation, early exit, model efficiency and optimization
- **Type**: Experimental research
- **Tools**: XLM-R, XNLI, WikiANN, PyTorch, Hugging Face, Transformers
- **Path**: `./thesis`

## ğŸ“‚ Structure
Each folder contains:
- ğŸ“˜ Project report or overview (PDF)
- ğŸ“ Supplementary materials (code/data snippets if applicable)
- ğŸ“„ README (optional, for detailed context)

---

## ğŸ”— Citation and Use
These works are shared for academic and portfolio purposes. If referencing, please credit the original authorship.

---

## ğŸ“¬ Contact
For questions or collaborations, feel free to reach out via GitHub or email me at:  
ğŸ“§ ludvig.xingming.li@gmail.com

